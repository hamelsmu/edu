# AUTOGENERATED! DO NOT EDIT! File to edit: evaluate.ipynb.

# %% auto 0
__all__ = ['run', 'fastai_model', 'train_df', 'test_df', 'valid_dls', 'val_metrics', 'label_func', 'get_data_split',
           'get_eval_artifacts']

# %% evaluate.ipynb 2
import wandb, params
import pandas as pd
from pathlib import Path
from fastai.vision.all import load_learner
from train import prepare_df

# %% evaluate.ipynb 3
def label_func(fname):
    return (fname.parent.parent/"labels")/f"{fname.stem}_mask.png"

def get_data_split(model_registry_artifact):
    "Get the validation and test dataset from the artifacts associated with the model from the registry."
    parent_run_artifacts = model_registry_artifact.logged_by().used_artifacts()
    datasets_used = [a for a in parent_run_artifacts if a.type == 'split_data']
    assert len(datasets_used) == 1, f'Only expected one input artifact with type == `split_data`, but found {len(datasets_used)}'
    data_artifact = datasets_used[0]
    artifact_dir = Path(data_artifact.download())
    return prepare_df(artifact_dir), prepare_df(artifact_dir, is_test=True)
    

def get_eval_artifacts(run, art_name='av-team/model-registry/BDD Semantic Segmentation:staging', art_type='model'):
    "Return the learner and the data split with the holdout set."
    artifact = run.use_artifact(art_name, type=art_type)
    artifact_dir = Path(artifact.download())
    model = load_learner(artifact_dir/'fastai_model.pkl')
    model.cuda()
    train_df, test_df = get_data_split(artifact)
   
    return model, train_df, test_df

# %% evaluate.ipynb 4
run = wandb.init(project=params.WANDB_PROJECT, 
                 entity=params.ENTITY, 
                 job_type="evaluation")

fastai_model, train_df, test_df = get_eval_artifacts(run)

# %% evaluate.ipynb 8
valid_dls = fastai_model.dls.test_dl(train_df[train_df.Stage == 'valid'], device=torch.device('cuda:0'))
val_metrics = fastai_model.validate(dl=valid_dls)
